var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Pre-Built-Architectures","page":"API Reference","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"NOMAD\nDeepONet\nFourierNeuralOperator","category":"page"},{"location":"api/#NeuralOperators.NOMAD","page":"API Reference","title":"NeuralOperators.NOMAD","text":"NOMAD(approximator, decoder, concatenate)\n\nConstructs a NOMAD from approximator and decoder architectures. Make sure the output from approximator combined with the coordinate dimension has compatible size for input to decoder\n\nArguments\n\napproximator: Lux network to be used as approximator net.\ndecoder: Lux network to be used as decoder net.\n\nKeyword Arguments\n\nconcatenate: function that defines the concatenation of output from approximator and the coordinate dimension, defaults to concatenation along first dimension after vectorizing the tensors\n\nReferences\n\n[1] Jacob H. Seidman and Georgios Kissas and Paris Perdikaris and George J. Pappas, \"NOMAD: Nonlinear Manifold Decoders for Operator Learning\", doi: https://arxiv.org/abs/2206.03551\n\nExample\n\njulia> approximator_net = Chain(Dense(8 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> decoder_net = Chain(Dense(18 => 16), Dense(16 => 16), Dense(16 => 8));\n\njulia> nomad = NOMAD(approximator_net, decoder_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), nomad);\n\njulia> u = rand(Float32, 8, 5);\n\njulia> y = rand(Float32, 2, 5);\n\njulia> size(first(nomad((u, y), ps, st)))\n(8, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.DeepONet","page":"API Reference","title":"NeuralOperators.DeepONet","text":"DeepONet(branch, trunk, additional)\n\nConstructs a DeepONet from a branch and trunk architectures. Make sure that both the nets output should have the same first dimension.\n\nArguments\n\nbranch: Lux network to be used as branch net.\ntrunk: Lux network to be used as trunk net.\n\nKeyword Arguments\n\nadditional: Lux network to pass the output of DeepONet, to include additional operations for embeddings, defaults to nothing\n\nReferences\n\n[1] Lu Lu, Pengzhan Jin, George Em Karniadakis, \"DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\", doi: https://arxiv.org/abs/1910.03193\n\nExample\n\njulia> branch_net = Chain(Dense(64 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> trunk_net = Chain(Dense(1 => 8), Dense(8 => 8), Dense(8 => 16));\n\njulia> deeponet = DeepONet(branch_net, trunk_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), deeponet);\n\njulia> u = rand(Float32, 64, 5);\n\njulia> y = rand(Float32, 1, 10, 5);\n\njulia> size(first(deeponet((u, y), ps, st)))\n(10, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.FourierNeuralOperator","page":"API Reference","title":"NeuralOperators.FourierNeuralOperator","text":"FourierNeuralOperator(\n    σ=gelu; chs::Dims{C}=(2, 64, 64, 64, 64, 64, 128, 1), modes::Dims{M}=(16,),\n    permuted::Val{perm}=False, kwargs...) where {C, M, perm}\n\nFourier neural operator is a operator learning model that uses Fourier kernel to perform spectral convolutions. It is a promising way for surrogate methods, and can be regarded as a physics operator.\n\nThe model is comprised of a Dense layer to lift (d + 1)-dimensional vector field to n-dimensional vector field, and an integral kernel operator which consists of four Fourier kernels, and two Dense layers to project data back to the scalar field of interest space.\n\nArguments\n\nσ: Activation function for all layers in the model.\n\nKeyword Arguments\n\nchs: A Tuple or Vector of the 8 channel size.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\npermuted: Whether the dim is permuted. If permuted = Val(false), the layer accepts data in the order of (ch, x_1, ... , x_d , batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nExample\n\njulia> fno = FourierNeuralOperator(gelu; chs=(2, 64, 64, 128, 1), modes=(16,));\n\njulia> ps, st = Lux.setup(Xoshiro(), fno);\n\njulia> u = rand(Float32, 2, 1024, 5);\n\njulia> size(first(fno(u, ps, st)))\n(1, 1024, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#Building-blocks","page":"API Reference","title":"Building blocks","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"OperatorConv\nSpectralConv\nOperatorKernel\nSpectralKernel","category":"page"},{"location":"api/#NeuralOperators.OperatorConv","page":"API Reference","title":"NeuralOperators.OperatorConv","text":"OperatorConv(ch::Pair{<:Integer, <:Integer}, modes::Dims,\n    ::Type{<:AbstractTransform}; init_weight=glorot_uniform,\n    permuted=Val(false))\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\n::Type{TR}: The transform to operate the transformation.\n\nKeyword Arguments\n\ninit_weight: Initial function to initialize parameters.\npermuted: Whether the dim is permuted. If permuted = Val(false), the layer accepts data in the order of (ch, x_1, ... , x_d, batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nExample\n\njulia> OperatorConv(2 => 5, (16,), FourierTransform{ComplexF32});\n\njulia> OperatorConv(2 => 5, (16,), FourierTransform{ComplexF32}; permuted=Val(true));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralConv","page":"API Reference","title":"NeuralOperators.SpectralConv","text":"SpectralConv(args...; kwargs...)\n\nConstruct a OperatorConv with FourierTransform{ComplexF32} as the transform. See OperatorConv for the individual arguments.\n\nExample\n\njulia> SpectralConv(2 => 5, (16,));\n\njulia> SpectralConv(2 => 5, (16,); permuted=Val(true));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#NeuralOperators.OperatorKernel","page":"API Reference","title":"NeuralOperators.OperatorKernel","text":"OperatorKernel(ch::Pair{<:Integer, <:Integer}, modes::Dims, transform::Type{TR},\n    act::A=identity; permuted=Val(false), kwargs...) where {TR <: AbstractTransform, A}\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\n::Type{TR}: The transform to operate the transformation.\n\nKeyword Arguments\n\nσ: Activation function.\npermuted: Whether the dim is permuted. If permuted = Val(true), the layer accepts data in the order of (ch, x_1, ... , x_d , batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nAll the keyword arguments are passed to the OperatorConv constructor.\n\nExample\n\njulia> OperatorKernel(2 => 5, (16,), FourierTransform{ComplexF64});\n\njulia> OperatorKernel(2 => 5, (16,), FourierTransform{ComplexF64}; permuted=Val(true));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralKernel","page":"API Reference","title":"NeuralOperators.SpectralKernel","text":"SpectralKernel(args...; kwargs...)\n\nConstruct a OperatorKernel with FourierTransform{ComplexF32} as the transform. See OperatorKernel for the individual arguments.\n\nExample\n\njulia> SpectralKernel(2 => 5, (16,));\n\njulia> SpectralKernel(2 => 5, (16,); permuted=Val(true));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#Transform-API","page":"API Reference","title":"Transform API","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"NeuralOperators.AbstractTransform","category":"page"},{"location":"api/#NeuralOperators.AbstractTransform","page":"API Reference","title":"NeuralOperators.AbstractTransform","text":"AbstractTransform\n\nInterface\n\nBase.ndims(<:AbstractTransform): N dims of modes\ntransform(<:AbstractTransform, x::AbstractArray): Apply the transform to x\ntruncate_modes(<:AbstractTransform, x_transformed::AbstractArray): Truncate modes that contribute to the noise\ninverse(<:AbstractTransform, x_transformed::AbstractArray): Apply the inverse transform to x_transformed\n\n\n\n\n\n","category":"type"},{"location":"tutorials/fno/#Fourier-Neural-Operators-(FNOs)","page":"FNO","title":"Fourier Neural Operators (FNOs)","text":"","category":"section"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"FNOs are a subclass of Neural Operators that learn the learn the kernel Kappa_theta, parameterized on theta between function spaces:","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"(Kappa_thetau)(x) = int_D kappa_theta(a(x) a(y) x y) dy  quad forall x in D","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"The kernel makes up a block v_t(x) which passes the information to the next block as:","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"v^(t+1)(x) = sigma((W^(t)v^(t) + Kappa^(t)v^(t))(x))","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"FNOs choose a specific kernel kappa(xy) = kappa(x-y), converting the kernel into a convolution operation, which can be efficiently computed in the fourier domain.","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"beginalign*\n(Kappa_thetau)(x) \n= int_D kappa_theta(x - y) dy  quad forall x in D\n= mathcalF^-1(mathcalF(kappa_theta) mathcalF(u))(x) quad forall x in D\nendalign*","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"where mathcalF denotes the fourier transform. Usually, not all the modes in the frequency domain are used with the higher modes often being truncated.","category":"page"},{"location":"tutorials/fno/#Usage","page":"FNO","title":"Usage","text":"","category":"section"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"u(x) = sin(alpha x)","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"That is, we want to learn","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"such that","category":"page"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"tutorials/fno/#Copy-pastable-code","page":"FNO","title":"Copy-pastable code","text":"","category":"section"},{"location":"tutorials/fno/","page":"FNO","title":"FNO","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\ndata_size = 128\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32;\nu_data = zeros(Float32, m, 1, data_size);\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size);\nv_data = zeros(Float32, m, 1, data_size);\n\nfor i in 1:data_size\n    u_data[:, 1, i] .= sin.(α[i] .* xrange)\n    v_data[:, 1, i] .= -inv(α[i]) .* cos.(α[i] .* xrange)\nend\n\nfno = FourierNeuralOperator(gelu; chs=(1, 64, 64, 128, 1), modes=(16,), permuted=Val(true))\n\nps, st = Lux.setup(rng, fno);\ndata = [(u_data, v_data)];\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.01f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(fno, ps, st, data; epochs=100)\n\nlines(losses)","category":"page"},{"location":"#NeuralOperators","page":"NeuralOperators.jl","title":"NeuralOperators","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"NeuralOperators.jl is a package written in Julia to provide the architectures for learning mapping between function spaces, and learning grid invariant solution of PDEs.","category":"page"},{"location":"#Installation","page":"NeuralOperators.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"On Julia 1.10+, you can install NeuralOperators.jl by running","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"import Pkg\nPkg.add(\"NeuralOperators\")","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Currently provided operator architectures are :","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Fourier Neural Operators (FNOs)\nDeepONets\nNonlinear Manifold Decoders for Operator Learning (NOMADs)","category":"page"},{"location":"#Reproducibility","page":"NeuralOperators.jl","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" *\n                name *\n                \".jl/tree/gh-pages/v\" *\n                version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" *\n               name *\n               \".jl/tree/gh-pages/v\" *\n               version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/deeponet/#DeepONets","page":"DeepONet","title":"DeepONets","text":"","category":"section"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"DeepONets are another class of networks that learn the mapping between two function spaces by encoding the input function space and the location of the output space. The latent code of the input space is then projected on the location laten code to give the output. This allows the network to learn the mapping between two functions defined on different spaces.","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"beginalign*\nu(y) xrightarrowtextbranch   b \n quad searrow\nquad quad mathcalG_theta u(y) = sum_k b_k t_k \n  quad nearrow \ny   xrightarrowtexttrunk    t  \nendalign*","category":"page"},{"location":"tutorials/deeponet/#Usage","page":"DeepONet","title":"Usage","text":"","category":"section"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"u(x) = sin(alpha x)","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"That is, we want to learn","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"such that","category":"page"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"tutorials/deeponet/#Copy-pastable-code","page":"DeepONet","title":"Copy-pastable code","text":"","category":"section"},{"location":"tutorials/deeponet/","page":"DeepONet","title":"DeepONet","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\neval_points = 1\ndata_size = 64\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nu_data = zeros(Float32, m, data_size)\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size)\n\ny_data = rand(Float32, 1, eval_points, data_size) .* 2π\nv_data = zeros(Float32, eval_points, data_size)\nfor i in 1:data_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[1, :, i])\nend\n\ndeeponet = DeepONet(\n    Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ)),\n    Chain(Dense(1 => 4, σ), Dense(4 => 8, σ))\n)\n\nps, st = Lux.setup(rng, deeponet)\ndata = [((u_data, y_data), v_data)]\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.001f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(deeponet, ps, st, data; epochs=1000)\n\nlines(losses)","category":"page"},{"location":"tutorials/nomad/#Nonlinear-Manifold-Decoders-for-Operator-Learning-(NOMADs)","page":"NOMAD","title":"Nonlinear Manifold Decoders for Operator Learning (NOMADs)","text":"","category":"section"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"NOMADs are similar to DeepONets in the aspect that they can learn when the input and output function spaces are defined on different domains. Their architecture is different and use nonlinearity to the latent codes to obtain the operator approximation. The architecture involves an approximator to encode the input function space, which is directly concatenated with the input function coordinates, and passed into a decoder net to give the output function at the given coordinate.","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"beginalign*\nu(y) xrightarrowmathcalA   beta \n quad searrow\n quad quad mathcalG_theta u(y) = mathcalD(beta y) \n quad nearrow \ny\nendalign*","category":"page"},{"location":"tutorials/nomad/#Usage","page":"NOMAD","title":"Usage","text":"","category":"section"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"u(x) = sin(alpha x)","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"That is, we want to learn","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"such that","category":"page"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"tutorials/nomad/#Copy-pastable-code","page":"NOMAD","title":"Copy-pastable code","text":"","category":"section"},{"location":"tutorials/nomad/","page":"NOMAD","title":"NOMAD","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\neval_points = 1\ndata_size = 128\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nu_data = zeros(Float32, m, data_size)\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size)\n\ny_data = rand(Float32, 1, eval_points, data_size) .* 2π\nv_data = zeros(Float32, eval_points, data_size)\nfor i in 1:data_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[1, :, i])\nend\n\nnomad = NOMAD(Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 7)),\n    Chain(Dense(8 => 4, σ), Dense(4 => 1)))\n\nps, st = Lux.setup(rng, nomad)\ndata = [((u_data, y_data), v_data)]\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.01f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(nomad, ps, st, data; epochs=1000)\n\nlines(losses)","category":"page"}]
}
